{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Documentation","text":"<p>MoPPy is a python library that implements several movement primitives for robotics.</p>"},{"location":"#1-testing-status","title":"1. Testing Status","text":""},{"location":"#2-getting-started","title":"2. Getting Started","text":"<p>Warning</p> <p>Moppy is currently a private Python package, which means it cannot be installed from the Python Package Index (PyPI) or other public repositories. Instead, you can install it directly from the GitHub repository.</p>"},{"location":"#21-install-the-library-from-github","title":"2.1 Install the Library from GitHub","text":""},{"location":"#211-install-the-library-from-github","title":"2.1.1 Install the Library from GitHub","text":"<p>To install Moppy, use the following command in your terminal. Make sure you have cloned the repository to your local machine or are in the root directory of the cloned repository.</p> <pre><code>pip install .\n</code></pre>"},{"location":"#212-continuous-development","title":"2.1.2 Continuous Development","text":"<p>If you're actively developing Moppy and want to make changes to the code, you can install it in \"editable\" mode using the -e flag. This allows you to modify the source code and have those changes reflected immediately without needing to reinstall the package.</p> <p>To install Moppy in editable mode, use the following command:</p> <pre><code>pip install -e .\n</code></pre>"},{"location":"#213-alternative-installing-directly-from-github","title":"2.1.3 Alternative: Installing Directly from GitHub","text":"<p>If you prefer to install Moppy directly from GitHub without cloning the repository, you can use the following command, replacing USERNAME with your GitHub username and REPO with the repository name:</p> <pre><code>pip install git+https://github.com/SamuelePolimi/moppy.git\n</code></pre> <p>You can now use this library in another project!</p>"},{"location":"#22-creating-the-conda-enviroment","title":"2.2 Creating the conda enviroment","text":"<p>If you want to use a conda enviroment, you can create the envirement using following command:</p> <pre><code>conda env create -f environment.yml\nconda activate moppy\n</code></pre>"},{"location":"#3-examples","title":"3. Examples","text":"<p>Take a look in the example folder, where you can find multiple implementations showcasing the functionality of the library.</p>"},{"location":"#4-testing","title":"4. Testing","text":"<ul> <li>All the tests can be found in the tests folder.</li> <li>There exists a ci for testings.</li> <li>For mor information on testing in moppy follow README.md.</li> </ul>"},{"location":"about/license/","title":"License Notice","text":""},{"location":"about/license/#important-information","title":"Important Information","text":"<p>This library is currently a private project and is not released under any formal license at this moment.</p> <p>For the latest updates and potential licensing information in the future, please refer to the GitHub repository.</p>"},{"location":"deep-promp/classes/","title":"Classes","text":""},{"location":"deep-promp/classes/#deeppromp","title":"DeepProMP","text":"<p><code>moppy.deep_promp.deep_promp.DeepProMP(name: str, encoder: EncoderDeepProMP, decoder: DecoderDeepProMP, save_path: str = './deep_promp/output/', log_to_tensorboard: bool = False, learning_rate: float = 0.005, epochs: int = 100, beta: float = 0.01)</code></p> <p>Bases: <code>moppy.interfaces.movement_primitive.MovementPrimitive</code></p>"},{"location":"deep-promp/classes/#parameters","title":"Parameters","text":"<ul> <li>name (<code>str</code>): Name of the model.</li> <li>encoder (<code>EncoderDeepProMP</code>): Encoder component for DeepProMP.</li> <li>decoder (<code>DecoderDeepProMP</code>): Decoder component for DeepProMP.</li> <li>save_path (<code>str</code>, optional): Path to save the output files. Defaults to <code>./deep_promp/output/</code>.</li> <li>log_to_tensorboard (<code>bool</code>, optional): Whether to log training to TensorBoard. Defaults to <code>False</code>.</li> <li>learning_rate (<code>float</code>, optional): Learning rate for the optimizer. Defaults to <code>0.005</code>.</li> <li>epochs (<code>int</code>, optional): Number of epochs for training. Defaults to <code>100</code>.</li> <li>beta (<code>float</code>, optional): Regularization parameter for the model. Defaults to <code>0.01</code>.</li> </ul>"},{"location":"deep-promp/classes/#functions","title":"Functions","text":""},{"location":"deep-promp/classes/#kl_annealing_scheduler","title":"kl_annealing_scheduler()Method SignatureParametersReturnsExample","text":"<p>The <code>kl_annealing_scheduler</code> is a static method used to adjust the Kullback-Leibler (KL) divergence term during training, commonly used in variational models. This annealing process helps in gradually increasing the weight of the KL divergence term, preventing the model from relying too heavily on the prior too early during training. The scheduler operates over multiple cycles and gradually saturates based on the provided <code>saturation_point</code>.</p> <p><code>kl_annealing_scheduler(current_epoch: int, n_cycles: int = 4, max_epoch: int = 1000, saturation_point: float = 0.5)</code></p> <ul> <li>current_epoch (<code>int</code>): The current epoch during training.</li> <li>n_cycles (<code>int</code>, optional): Number of cycles to repeat the annealing process. Defaults to <code>4</code>.</li> <li>max_epoch (<code>int</code>, optional): Maximum number of training epochs. Defaults to <code>1000</code>.</li> <li>saturation_point (<code>float</code>, optional): The point at which the annealing process saturates (i.e., reaches its maximum). Defaults to <code>0.5</code>.</li> </ul> <ul> <li>tau (<code>float</code>): A value between <code>0</code> and <code>1</code>, representing the current weight of the KL divergence term.</li> </ul> <pre><code># Using the KL annealing scheduler in training\nkl_weight = DeepProMP.kl_annealing_scheduler(current_epoch=50)\n</code></pre>"},{"location":"deep-promp/classes/#gauss_kl","title":"gauss_kl()Method SignatureParametersReturnsExample","text":"<p>The <code>gauss_kl</code> method calculates the Kullback-Leibler (KL) divergence between a given Gaussian distribution (with parameters <code>mu_q</code> and <code>std_q</code>) and a standard Gaussian distribution. This divergence is a measure of how one probability distribution diverges from a second, reference distribution (in this case, the standard Gaussian). It's a key component of variational inference models.</p> <p><code>gauss_kl(mu_q: torch.Tensor, std_q: torch.Tensor) -&gt; torch.Tensor</code></p> <ul> <li>mu_q (<code>torch.Tensor</code>): The mean of the approximate posterior distribution.</li> <li>std_q (<code>torch.Tensor</code>): The standard deviation of the approximate posterior distribution.</li> </ul> <ul> <li>kl_divergence (<code>torch.Tensor</code>): The mean KL divergence between the approximate posterior and the standard Gaussian distribution.</li> </ul> <pre><code># Calculate KL divergence for a Gaussian distribution with given mean and std deviation\nkl_div = DeepProMP.gauss_kl(mu_q=mu_tensor, std_q=std_tensor)\n</code></pre>"},{"location":"deep-promp/classes/#calculate_elbo","title":"calculate_elbo()Method SignatureParametersReturnsExample","text":"<p>The <code>calculate_elbo</code> method computes the Evidence Lower Bound (ELBO), which is the objective function used to train variational models like DeepProMP. The ELBO consists of two main components: the reconstruction loss (typically Mean Squared Error) and the KL divergence, weighted by a factor <code>beta</code>. The goal of ELBO is to balance between fitting the data and keeping the posterior distribution close to the prior.</p> <p><code>calculate_elbo(y_pred: torch.Tensor, y_star: torch.Tensor, mu: torch.Tensor, sigma: torch.Tensor, beta: float = 1.0) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor]</code></p> <ul> <li>y_pred (<code>torch.Tensor</code>): The predicted output from the model.</li> <li>y_star (<code>torch.Tensor</code>): The ground truth target values.</li> <li>mu (<code>torch.Tensor</code>): The mean of the approximate posterior distribution.</li> <li>sigma (<code>torch.Tensor</code>): The standard deviation of the approximate posterior distribution.</li> <li>beta (<code>float</code>, optional): The weight applied to the KL divergence term in the ELBO. Defaults to <code>1.0</code>.</li> </ul> <ul> <li>elbo (<code>torch.Tensor</code>): The total ELBO loss, combining the reconstruction loss and KL divergence.</li> <li>mse (<code>torch.Tensor</code>): The Mean Squared Error reconstruction loss.</li> <li>kl (<code>torch.Tensor</code>): The KL divergence between the approximate posterior and the prior.</li> </ul> <pre><code># Calculate ELBO during training\nelbo_loss, mse_loss, kl_div = DeepProMP.calculate_elbo(y_pred=pred, y_star=target, mu=mu_tensor, sigma=std_tensor)\n</code></pre>"},{"location":"deep-promp/classes/#train","title":"train()Method SignatureParametersReturnsDescriptionExample","text":"<p>The <code>train</code> method is responsible for training the <code>DeepProMP</code> model using the provided trajectories. The training process is guided by the Evidence Lower Bound (ELBO) as the loss function. It leverages the Adam optimizer for updating model parameters and supports KL annealing for better regularization during training. The method divides the data into training and validation sets, logs metrics, and saves the model and losses after training.</p> <p><code>train(trajectories: List[Trajectory], kl_annealing: bool = True, beta: float = None, learning_rate: float = None, epochs: int = None) -&gt; None</code></p> <ul> <li>trajectories (<code>List[Trajectory]</code>): A list of trajectory data used for training the model.</li> <li>kl_annealing (<code>bool</code>, optional): If <code>True</code>, applies KL annealing during training. Defaults to <code>True</code>.</li> <li>beta (<code>float</code>, optional): The regularization weight for the KL divergence term. If not specified, uses the default value in the class.</li> <li>learning_rate (<code>float</code>, optional): The learning rate for the optimizer. If not specified, uses the default value in the class.</li> <li>epochs (<code>int</code>, optional): The number of training epochs. If not specified, uses the default value in the class.</li> </ul> <ul> <li>None</li> </ul> <p>The <code>train</code> method performs the following steps:</p> <ol> <li>Optionally adjusts the <code>beta</code>, <code>learning_rate</code>, and <code>epochs</code> values based on the provided arguments.</li> <li>Divides the input <code>trajectories</code> into training and validation sets.</li> <li>Initializes the Adam optimizer to update both the encoder and decoder parameters.</li> <li>For each epoch:<ul> <li>Loops over the training set, computes the ELBO loss (a combination of Mean Squared Error and KL divergence), and updates the model.</li> <li>If KL annealing is enabled, it applies a scheduling function to gradually increase the weight of the KL divergence term over training.</li> <li>Logs the metrics for each epoch.</li> <li>Validates the model on the validation set.</li> </ul> </li> <li>After training is completed:<ul> <li>Saves the training losses, validation losses, and models.</li> <li>Generates loss plots.</li> </ul> </li> </ol> <pre><code># Train the DeepProMP model with a list of trajectories\nmodel = DeepProMP(name=\"example_model\", encoder=encoder, decoder=decoder)\nmodel.train(trajectories=trajectories_list, kl_annealing=True, beta=0.01, learning_rate=0.001, epochs=100)\n</code></pre>"},{"location":"deep-promp/classes/#validate","title":"validate()Method SignatureParametersReturnsDescriptionExample","text":"<p>The <code>validate</code> method evaluates the performance of the trained <code>DeepProMP</code> model on a set of trajectories. It computes the Mean Squared Error (MSE) between the predicted (decoded) output and the actual trajectory data for each trajectory in the validation set. The average loss over all validation trajectories is returned as the validation loss.</p> <p><code>validate(trajectories: List[Trajectory]) -&gt; float</code></p> <ul> <li>trajectories (<code>List[Trajectory]</code>): A list of trajectory data used for validation.</li> </ul> <ul> <li>validation_loss (<code>float</code>): The average Mean Squared Error (MSE) loss over all validation trajectories.</li> </ul> <p>The <code>validate</code> method performs the following steps:</p> <ol> <li>For each trajectory in the validation set:<ul> <li>Passes the trajectory through the encoder to obtain the posterior distribution parameters (<code>mu</code> and <code>sigma</code>).</li> <li>Samples the latent variable <code>z</code> from the posterior.</li> <li>Decodes the latent variable at each time point to reconstruct the trajectory.</li> <li>Computes the Mean Squared Error (MSE) between the reconstructed trajectory and the ground truth trajectory.</li> </ul> </li> <li>Averages the MSE loss over all validation trajectories.</li> <li>Returns the average validation loss.</li> </ol> <pre><code># Validate the DeepProMP model on a list of trajectories\nvalidation_loss = model.validate(trajectories=validation_trajectories_list)\nprint(f\"Validation Loss: {validation_loss}\")\n</code></pre>"},{"location":"deep-promp/classes/#save_models","title":"save_models()Method SignatureParameters:Example:","text":"<p>The <code>save_models</code> method saves the encoder and decoder models to the specified path. If no path is provided, it uses the default save path.</p> <p><code>save_models(save_path: str = None) -&gt; None</code></p> <ul> <li>save_path (<code>str</code>, optional): The path where the models should be saved. If <code>None</code>, the default <code>save_path</code> is used.</li> </ul> <pre><code># Save models to the default path\nmodel.save_models()\n\n# Save models to a custom path\nmodel.save_models(save_path='./custom_path/')\n</code></pre>"},{"location":"deep-promp/classes/#save_losses","title":"save_losses()Method SignatureParametersDescriptionExample","text":"<p>The <code>save_losses</code> method saves the losses (training, validation, KL divergence, MSE) to the specified path. If no path is provided, it uses the default save path.</p> <p><code>save_losses(save_path: str = None) -&gt; None</code></p> <ul> <li>save_path (<code>str</code>, optional): The path where the losses should be saved. If <code>None</code>, the default <code>save_path</code> is used.</li> </ul> <p>This method saves the following loss values to disk:</p> <ul> <li>Validation loss (<code>validation_loss.pth</code>)</li> <li>KL divergence loss (<code>kl_loss.pth</code>)</li> <li>Mean Squared Error loss (<code>mse_loss.pth</code>)</li> <li>Training loss (<code>train_loss.pth</code>)</li> </ul> <p>For each type of loss, the method saves the values using the <code>torch.save()</code> function.</p> <pre><code># Save losses to the default path\nmodel.save_losses()\n\n# Save losses to a custom path\nmodel.save_losses(save_path='./custom_path/')\n</code></pre>"},{"location":"deep-promp/classes/#plot_values","title":"plot_values()Method SignatureParametersExample","text":"<p>The <code>plot_values</code> method plots the provided values and saves the plot to the specified path. If no path is provided, it uses the default save path.</p> <p><code>plot_values(values: List[List], file_name: str, plot_title: str = \"Plot\", path: str = None) -&gt; None</code></p> <ul> <li>values (<code>List[List]</code>): The values to be plotted. Each inner list represents a line in the plot.</li> <li>file_name (<code>str</code>): The name of the file where the plot will be saved.</li> <li>plot_title (<code>str</code>, optional): The title of the plot. Defaults to <code>\"Plot\"</code>.</li> <li>path (<code>str</code>, optional): The path where the plot should be saved. If <code>None</code>, the default <code>save_path</code> is used.</li> </ul> <pre><code># Plot the values and save to the default path\nmodel.plot_values(values=[[1, 2, 3], [4, 5, 6]], file_name='example_plot.png', plot_title=\"Example Plot\")\n\n# Plot the values and save to a custom path\nmodel.plot_values(values=[[1, 2, 3], [4, 5, 6]], file_name='example_plot.png', plot_title=\"Example Plot\", path='./custom_path/')\n</code></pre>"},{"location":"deep-promp/classes/#decoderdeeppromp","title":"DecoderDeepProMP","text":"<p><code>moppy.deep_promp.decoder_deep_pro_mp.DeepProMP(self, latent_variable_dimension: int, hidden_neurons: List[int], trajectory_state_class: Type[TrajectoryState] = JointConfiguration, activation_function: Type[nn.Module] = nn.ReLU, activation_function_params: dict = {})</code></p> <p>Bases: <code>moppy.interfaces.movement_primitive.LatentDecoder</code></p> <p>The <code>DecoderDeepProMP</code> class implements a latent decoder architecture, extending <code>LatentDecoder</code> and <code>nn.Module</code>. It is designed to decode a latent variable into a trajectory state.</p>"},{"location":"deep-promp/classes/#parameters_1","title":"Parameters","text":"<ul> <li>latent_variable_dimension (<code>int</code>): The dimension of the latent variable.</li> <li>hidden_neurons (<code>List[int]</code>): A list of integers representing the number of neurons in each hidden layer.</li> <li>trajectory_state_class (<code>Type[TrajectoryState]</code>): The class of the trajectory state (default: <code>JointConfiguration</code>).</li> <li>activation_function (<code>Type[nn.Module]</code>): The activation function to be used in the network (default: <code>nn.ReLU</code>).</li> <li>activation_function_params (<code>dict</code>): Parameters for the activation function.</li> </ul> Raises: <ul> <li>TypeError: If <code>trajectory_state_class</code> is not a subclass of <code>TrajectoryState</code>.</li> <li>ValueError: If <code>latent_variable_dimension</code> is less than or equal to 0, if the number of neurons is less than 2, or if any neuron count is not greater than 0.</li> </ul>"},{"location":"deep-promp/classes/#functions_1","title":"Functions","text":""},{"location":"deep-promp/classes/#load_from_save_file","title":"load_from_save_file()Method SignatureParameters:Returns:","text":"<p>Loads a model from a file and returns a <code>DecoderDeepProMP</code> instance. It uses a the savefile created by <code>save_decoder</code>.</p> <p><code>load_from_save_file(cls, path: str = '', file: str = \"decoder_deep_pro_mp.pth\") -&gt; 'DecoderDeepProMP'</code></p> <ul> <li>path (<code>str</code>): The path to the directory containing the model file.</li> <li>file (<code>str</code>): The name of the file to load (default: <code>\"decoder_deep_pro_mp.pth\"</code>).</li> </ul> <ul> <li>DecoderDeepProMP: An instance of <code>DecoderDeepProMP</code>.</li> </ul>"},{"location":"deep-promp/classes/#create_layers","title":"create_layers()","text":"<p>Creates the layers of the decoder network based on the specified number of neurons.</p> <p>Returns:</p> <ul> <li>List[nn.Module]: A list of layers for the neural network.</li> </ul>"},{"location":"deep-promp/classes/#__init_weights","title":"__init_weights()Method SignatureParameters:","text":"<p>Initializes the weights and biases of the network using Xavier initialization.</p> <p><code>__init_weights(self, m) -&gt; None</code></p> <ul> <li>m: The layer to initialize.</li> </ul>"},{"location":"deep-promp/classes/#decode_from_latent_variable","title":"decode_from_latent_variable()Method SignatureParameters:Returns:","text":"<p>Overrides: moppy.interfaces.latent_decoder.LatentDecoder.decode_from_latent_variable</p> <p>Decodes a latent variable into a tensor representing the trajectory state.</p> <p><code>decode_from_latent_variable(self, latent_variable: torch.Tensor, time: Union[torch.Tensor, float]) -&gt; torch.Tensor</code></p> <ul> <li>latent_variable (<code>torch.Tensor</code>): The latent variable to decode.</li> <li>time (<code>Union[torch.Tensor, float]</code>): The normalized time value.</li> </ul> <ul> <li>torch.Tensor: The decoded trajectory state.</li> </ul>"},{"location":"deep-promp/classes/#save_decoder","title":"save_decoder()","text":"<p>Saves the decoder model to a file, including the state dictionary and configuration.</p> <p>Method Signature</p> <p><code>save_decoder(self, path: str = '', filename: str = \"decoder_deep_pro_mp.pth\")</code></p> <p>Parameters:</p> <ul> <li>path (<code>str</code>): The directory to save the model.</li> <li>filename (<code>str</code>): The name of the file to save (default: <code>\"decoder_deep_pro_mp.pth\"</code>).</li> </ul>"},{"location":"deep-promp/classes/#save_model","title":"save_model()","text":"<p>Saves only the model's state dictionary.</p> <p>Method Signature</p> <p><code>save_model(self, path: str = '', filename: str = \"decoder_model_deep_pro_mp.pth\")</code></p> <p>Parameters:</p> <ul> <li>path (<code>str</code>): The directory to save the model.</li> <li>filename (<code>str</code>): The name of the file to save (default: <code>\"decoder_model_deep_pro_mp.pth\"</code>).</li> </ul>"},{"location":"deep-promp/classes/#load_model","title":"load_model()","text":"<p>Loads the model's state dictionary (<code>net.state_dict()</code>) from a file. The save_file can be created by created by <code>save_model</code>.</p> <p>Method Signature</p> <p><code>load_model(self, path: str = '', filename: str = \"decoder_model_deep_pro_mp.pth\")</code></p> <p>Parameters:</p> <ul> <li>path (<code>str</code>): The directory containing the model file.</li> <li>filename (<code>str</code>): The name of the file to load (default: <code>\"decoder_model_deep_pro_mp.pth\"</code>).</li> </ul>"},{"location":"deep-promp/classes/#forward","title":"forward()","text":"<p>Defines the forward pass of the decoder.</p> <p>Method Signature</p> <p><code>forward(self, latent_variable: torch.Tensor, time: Union[torch.Tensor, float])</code></p> <p>Parameters:</p> <ul> <li>latent_variable (<code>torch.Tensor</code>): The latent variable to decode.</li> <li>time (<code>Union[torch.Tensor, float]</code>): The normalized time value.</li> </ul> <p>Returns:</p> <ul> <li>torch.Tensor: The decoded trajectory state.</li> </ul>"},{"location":"deep-promp/classes/#encoderdeeppromp","title":"EncoderDeepProMP","text":"<p><code>moppy.deep_promp.encoder_deep_pro_mp.EncoderDeepProMP(self, latent_variable_dimension: int, hidden_neurons: List[int], trajectory_state_class: Type[TrajectoryState] = JointConfiguration, activation_function: Type[nn.Module] = nn.ReLU, activation_function_params: dict = {})</code></p> <p>Bases: <code>moppy.interfaces.movement_primitive.LatentEncoder</code></p> <p>The <code>EncoderDeepProMP</code> class implements a latent encoder architecture, extending <code>LatentEncoder</code> and <code>nn.Module</code>. It is designed to encode a trajectory into a latent variable.</p>"},{"location":"deep-promp/classes/#parameters_2","title":"Parameters","text":"<ul> <li>latent_variable_dimension (<code>int</code>): The dimension of the latent variable.</li> <li>hidden_neurons (<code>List[int]</code>): A list of integers representing the number of neurons in each hidden layer.</li> <li>trajectory_state_class (<code>Type[TrajectoryState]</code>): The class of the trajectory state (default: <code>JointConfiguration</code>).</li> <li>activation_function (<code>Type[nn.Module]</code>): The activation function to be used in the network (default: <code>nn.ReLU</code>).</li> <li>activation_function_params (<code>dict</code>): Parameters for the activation function.</li> </ul> Raises: <ul> <li>TypeError: If <code>trajectory_state_class</code> is not a subclass of <code>TrajectoryState</code>.</li> <li>ValueError: If <code>latent_variable_dimension</code> is less than or equal to 0, if the number of neurons is less than 2, or if any neuron count is not greater than 0.</li> </ul>"},{"location":"deep-promp/classes/#functions_2","title":"Functions","text":""},{"location":"deep-promp/classes/#load_from_save_file_1","title":"load_from_save_file()Method SignatureParameters:Returns:","text":"<p>Loads a model from a file and returns an <code>EncoderDeepProMP</code> instance. It uses a save file created by <code>save_encoder</code>.</p> <p><code>load_from_save_file(cls, path: str = '', file: str = \"encoder_deep_pro_mp.pth\") -&gt; 'EncoderDeepProMP'</code></p> <ul> <li>path (<code>str</code>): The path to the directory containing the model file.</li> <li>file (<code>str</code>): The name of the file to load (default: <code>\"encoder_deep_pro_mp.pth\"</code>).</li> </ul> <ul> <li>EncoderDeepProMP: An instance of <code>EncoderDeepProMP</code>.</li> </ul>"},{"location":"deep-promp/classes/#create_layers_1","title":"create_layers()","text":"<p>Creates the layers of the encoder network based on the specified number of neurons.</p> <p>Returns:</p> <ul> <li>List[nn.Module]: A list of layers for the neural network.</li> </ul>"},{"location":"deep-promp/classes/#__init_weights_1","title":"__init_weights()Method SignatureParameters:","text":"<p>Initializes the weights and biases of the network using Xavier initialization.</p> <p><code>__init_weights(self, m) -&gt; None</code></p> <ul> <li>m: The layer to initialize.</li> </ul>"},{"location":"deep-promp/classes/#encode_to_latent_variable","title":"encode_to_latent_variable()Method SignatureParameters:Returns:","text":"<p>Encodes a trajectory into a mu and sigma (both with dimensions of <code>latent_variable_dimension</code>).</p> <p><code>encode_to_latent_variable(self, trajectory: Trajectory) -&gt; tuple[Tensor, Tensor]</code></p> <ul> <li>trajectory (<code>Trajectory</code>): The trajectory to encode.</li> </ul> <ul> <li>tuple[Tensor, Tensor]: The resulting <code>mu</code> and <code>sigma</code> tensors, each with size <code>latent_variable_dimension</code>.</li> </ul>"},{"location":"deep-promp/classes/#sample_latent_variable","title":"sample_latent_variable()Method SignatureParameters:Returns:","text":"<p>Samples a latent variable <code>z</code> from a normal distribution specified by <code>mu</code> and <code>sigma</code>.</p> <p><code>sample_latent_variable(self, mu: torch.Tensor, sigma: torch.Tensor, percentage_of_standard_deviation=None) -&gt; torch.Tensor</code></p> <ul> <li>mu (<code>torch.Tensor</code>): The mean tensor.</li> <li>sigma (<code>torch.Tensor</code>): The standard deviation tensor.</li> <li>percentage_of_standard_deviation (<code>Optional[float]</code>): A percentage to scale the standard deviation (optional).</li> </ul> <ul> <li>torch.Tensor: The sampled latent variable.</li> </ul>"},{"location":"deep-promp/classes/#sample_latent_variables","title":"sample_latent_variables()Method SignatureParameters:Returns:","text":"<p>Samples multiple latent variables from given <code>mu</code> and <code>sigma</code> tensors.</p> <p><code>sample_latent_variables(self, mu: torch.Tensor, sigma: torch.Tensor, size: int = 1) -&gt; torch.Tensor</code></p> <ul> <li>mu (<code>torch.Tensor</code>): The mean tensor.</li> <li>sigma (<code>torch.Tensor</code>): The standard deviation tensor.</li> <li>size (<code>int</code>): The number of samples to generate (default: <code>1</code>).</li> </ul> <ul> <li>torch.Tensor: A tensor containing the sampled latent variables.</li> </ul>"},{"location":"deep-promp/classes/#bayesian_aggregation","title":"bayesian_aggregation()Method SignatureParameters:Returns:","text":"<p>Performs Bayesian aggregation on <code>mu_points</code> and <code>sigma_points</code> to calculate aggregated <code>mu</code> and <code>sigma</code>.</p> <p><code>bayesian_aggregation(self, mu_points: torch.Tensor, sigma_points: torch.Tensor) -&gt; tuple[torch.Tensor, torch.Tensor]</code></p> <ul> <li>mu_points (<code>torch.Tensor</code>): Tensor of <code>mu</code> points.</li> <li>sigma_points (<code>torch.Tensor</code>): Tensor of <code>sigma</code> points.</li> </ul> <ul> <li>tuple[torch.Tensor, torch.Tensor]: Aggregated <code>mu_z</code> and <code>sigma_z_sq</code> tensors.</li> </ul>"},{"location":"deep-promp/classes/#save_encoder","title":"save_encoder()","text":"<p>Saves the encoder model to a file, including the state dictionary and configuration.</p> <p>Method Signature</p> <p><code>save_encoder(self, path: str = '', filename: str = \"encoder_deep_pro_mp.pth\")</code></p> <p>Parameters:</p> <ul> <li>path (<code>str</code>): The directory to save the model.</li> <li>filename (<code>str</code>): The name of the file to save (default: <code>\"encoder_deep_pro_mp.pth\"</code>).</li> </ul>"},{"location":"deep-promp/classes/#save_model_1","title":"save_model()","text":"<p>Saves only the model's state dictionary.</p> <p>Method Signature</p> <p><code>save_model(self, path: str = '', filename: str = \"encoder_model_deep_pro_mp.pth\")</code></p> <p>Parameters:</p> <ul> <li>path (<code>str</code>): The directory to save the model.</li> <li>filename (<code>str</code>): The name of the file to save (default: <code>\"encoder_model_deep_pro_mp.pth\"</code>).</li> </ul>"},{"location":"deep-promp/classes/#load_model_1","title":"load_model()","text":"<p>Loads the model's state dictionary from a file.</p> <p>Method Signature</p> <p><code>load_model(self, path: str = '', filename: str = \"encoder_model_deep_pro_mp.pth\")</code></p> <p>Parameters:</p> <ul> <li>path (<code>str</code>): The directory containing the model file.</li> <li>filename (<code>str</code>): The name of the file to load (default: <code>\"encoder_model_deep_pro_mp.pth\"</code>).</li> </ul>"},{"location":"deep-promp/classes/#forward_1","title":"forward()","text":"<p>Defines the forward pass of the encoder.</p> <p>Method Signature</p> <p><code>forward(self, trajectory: Trajectory) -&gt; tuple[Tensor, Tensor]</code></p> <p>Parameters:</p> <ul> <li>trajectory (<code>Trajectory</code>): The trajectory to encode.</li> </ul> <p>Returns:</p> <ul> <li>tuple[Tensor, Tensor]: The encoded <code>mu</code> and <code>sigma</code> tensors.</li> </ul>"},{"location":"deep-promp/overview/","title":"Deep Pro MP","text":"<p>Probabilistic Movement Primitives (ProMPs) offer a powerful framework for robot skill learning by representing movements as distributions of trajectories.</p> <p>In this part of the library we use neural network to create Probabilistic Movement Primitives, thus the name Deep Pro MPs.</p>"},{"location":"deep-promp/overview/#structure","title":"Structure","text":"<p>The main part is the class DeepProMP it implements the core training algorithm. It used a DecoderDeepProMP and a EncoderDeepProMP.</p>"},{"location":"deep-promp/usage/","title":"Usage Example","text":""},{"location":"deep-promp/usage/#training-deeppromp-on-sinusoidal-trajectories","title":"Training DeepProMP on Sinusoidal Trajectories","text":"<p>This example demonstrates how to set up, train, and use the DeepProMP model on a set of sinusoidal trajectories using <code>moppy</code>.</p>"},{"location":"deep-promp/usage/#overview","title":"Overview","text":"<p>In this tutorial, we will:</p> <ol> <li>Set up an encoder and decoder using DeepProMP.</li> <li>Train DeepProMP on a sinusoidal trajectory set.</li> <li>Autoencode a sinusoidal trajectory and compare the results with the original.</li> </ol>"},{"location":"deep-promp/usage/#code-example","title":"Code Example","text":"<p>You can view the full code for this example in the GitHub repository.</p> <p>Below is the core code used for generating and training the model on sinusoidal trajectories.</p> <pre><code>import os\nfrom typing import List\nimport torch\n\n# Moppy imports\nfrom moppy.deep_promp import (\n    DeepProMP, EncoderDeepProMP, DecoderDeepProMP,\n    plot_trajectories, generate_sin_trajectory, generate_sin_trajectory_set)\nfrom moppy.trajectory.state import SinusState\nfrom moppy.trajectory.trajectory import Trajectory\n\nsave_path = './results_small_sinus_example/'\n\ndef main():\n    print(\"Starting the DeepProMP example...\\n\")\n    # Define the DecoderDeepProMP\n    decoder = DecoderDeepProMP(latent_variable_dimension=2,\n                               hidden_neurons=[10, 20, 30, 20, 10],\n                               trajectory_state_class=SinusState)\n\n    # Define the EncoderDeepProMP\n    encoder = EncoderDeepProMP(latent_variable_dimension=2,\n                               hidden_neurons=[10, 20, 30, 20, 10],\n                               trajectory_state_class=SinusState)\n\n    # Define the DeepProMP\n    deep_pro_mp = DeepProMP(name=\"sinus_main\",\n                            encoder=encoder,\n                            decoder=decoder,\n                            learning_rate=0.005,\n                            epochs=50,\n                            save_path=save_path)\n\n    print(120 * \"#\")\n    print(\"This example will train a DeepProMP with sinusoidal trajectories and then autoencode a sinusoidal trajectory:\")\n    print(f\"\\t - For this example we use the TrajectoryState class {decoder.trajectory_state_class.__name__}\")\n    print(f\"\\t - Encoder with {encoder.latent_variable_dimension} latent variables and {encoder.neurons} neurons\")\n    print(f\"\\t - Decoder with {decoder.latent_variable_dimension} latent variables and {decoder.neurons} neurons\")\n    print(f\"\\t - DeepProMP with {deep_pro_mp.epochs} epochs and learning rate of {deep_pro_mp.learning_rate}\")\n    print(120 * \"#\")\n\n    # Generate a set of sinusoidal trajectories\n    traj_set: List[Trajectory[SinusState]] = generate_sin_trajectory_set(50)\n\n    # Train the DeepProMP\n    deep_pro_mp.train(traj_set)\n\n    # Autoencode a sinusoidal trajectory and compare with the original\n    amplitude, frequency = 5, 1\n    traj = generate_sin_trajectory(amplitude, frequency)\n    mu, sigma = encoder.encode_to_latent_variable(traj)\n    latent_variable_z = encoder.sample_latent_variable(mu, sigma)\n    autoencoded_trajectory = Trajectory[SinusState]()\n\n    for point in traj.get_points():\n        t = point.get_time()\n        state = decoder.decode_from_latent_variable(latent_variable=latent_variable_z, time=t)\n        sinus_state = SinusState.from_vector_without_time(vector=state, time=t)\n        autoencoded_trajectory.add_point(sinus_state)\n\n    # Plot the original and autoencoded trajectory\n    file_path = os.path.join(save_path, \"Trajectories_Comparison.png\")\n    plot_trajectories(\n        labeled_trajectories=[(traj, \"Generated Trajectory\"), (autoencoded_trajectory, \"Autoencoded Trajectory\")],\n        file_name=file_path,\n        plot_title=\"Original vs Autoencoded Trajectory\")\n\n    print(f\"Trajectories saved at {file_path}\")\n    print(\"Done!\")\n\nif __name__ == '__main__':\n    main()\n</code></pre>"},{"location":"deep-promp/usage/#console-output","title":"Console Output","text":"<p>The output below is what you should expect in the terminal during execution. The training progress will show validation and training loss for each epoch.</p> <pre><code>Starting the DeepProMP example...\n\n########################################################################################################################\nThis example will train a DeepProMP with sinusoidal trajectories and then autoencode a sinusoidal trajectory:\n         - For this example we use the TrajectoryState class SinusState\n         - Encoder with 2 latent variables and [2, 10, 20, 30, 20, 10, 4] neurons\n         - Decoder with 2 latent variables and [3, 10, 20, 30, 20, 10, 1] neurons\n         - DeepProMP with 50 epochs and learning rate of 0.005\n########################################################################################################################\nStart DeepProMP training ...\nTotal set:  50\nTraining set: 45\nValidation set: 5\nEpoch  1/50 (2.35s): validation loss = 14.3365, train_loss = 14.9515, mse = 14.9515, kl = 5.8765\n...\nEpoch 50/50 (2.23s): validation loss = 0.3477, train_loss = 0.5100, mse = 0.5100, kl = 5.7649\nTraining finished (Time = 134.91s).\nSaving losses ...finished.\nPlotting...finished.\nSaving models...finished.\nTrajectories saved at ./results_small_sinus_example/Trajectories_Comparison.png\nDone!\n</code></pre>"},{"location":"deep-promp/usage/#resulting-files","title":"Resulting Files","text":"<p>After running the example, you will find the following files in the output directory (<code>./results_small_sinus_example/</code>):</p> <pre><code>.\n\u251c\u2500\u2500 all_losses.png\n\u251c\u2500\u2500 decoder_deep_pro_mp.pth\n\u251c\u2500\u2500 decoder_model_deep_pro_mp.pth\n\u251c\u2500\u2500 encoder_deep_pro_mp.pth\n\u251c\u2500\u2500 encoder_model_deep_pro_mp.pth\n\u251c\u2500\u2500 kl_loss.png\n\u251c\u2500\u2500 kl_loss.pth\n\u251c\u2500\u2500 mse_loss.pth\n\u251c\u2500\u2500 ms_loss.png\n\u251c\u2500\u2500 train_loss.png\n\u251c\u2500\u2500 train_loss.pth\n\u251c\u2500\u2500 Trajectories_Comparison.png\n\u251c\u2500\u2500 validation_loss.png\n\u2514\u2500\u2500 validation_loss.pth\n</code></pre> <p>Each file serves a purpose:</p> <ul> <li>Model Files (<code>encoder_deep_pro_mp.pth</code>, <code>decoder_deep_pro_mp.pth</code>): Contains trained model parameters for reuse.</li> <li>Loss Files (<code>train_loss.png</code>, <code>validation_loss.png</code>, <code>kl_loss.png</code>, etc.): Visualize training progress for key metrics, including MSE and KL divergence.</li> <li>Combined Loss Plot (<code>all_losses.png</code>): Consolidates all key loss metrics\u2014training, validation, MSE, and KL divergence\u2014into a single plot for easier monitoring.</li> <li>Trajectory Comparison (<code>Trajectories_Comparison.png</code>): Plot comparing the original and autoencoded trajectories to assess model performance.</li> </ul>"},{"location":"deep-promp/usage/#visualizations","title":"Visualizations","text":"<ul> <li>Trajectory Comparison: The final plot (<code>Trajectories_Comparison.png</code>) displays the original trajectory and the autoencoded trajectory side by side. This comparison shows how accurately DeepProMP reconstructs sinusoidal patterns from latent representations.</li> </ul> <ul> <li>Loss Metrics: The combined loss plot (<code>all_losses.png</code>) shows the training and validation loss progress over each epoch, along with MSE and KL divergence values. This comprehensive view helps assess the model's convergence and stability.</li> </ul>"},{"location":"deep-promp/usage/#additional-notes","title":"Additional Notes","text":"<ul> <li>Custom Trajectories: This example uses a fixed-frequency sinusoidal wave. To experiment with varying frequencies, you can modify the <code>generate_sin_trajectory_set</code> function.</li> <li>Parameter Adjustments: Adjusting the latent variable dimensions, learning rate, and network architecture (e.g., number of neurons per layer) can enhance model performance based on the trajectory patterns you wish to model.</li> </ul>"},{"location":"deep-promp/usage/#example-code","title":"Example Code","text":"<p>You can view this example and access the full code in the GitHub repository.</p> <p>This guide provides an overview of DeepProMP\u2019s capabilities using sinusoidal trajectories. Experiment with different configurations and trajectory shapes to fully leverage the model\u2019s potential for trajectory-based tasks.</p>"},{"location":"deep-promp/util/","title":"DeepProMP Utility Functions","text":""},{"location":"deep-promp/util/#set_seed","title":"set_seed()","text":"<p>The <code>set_seed</code> function sets the random seed for Python, NumPy, and PyTorch, enabling reproducibility across experiments by controlling the randomness in operations.</p> <p>Method Signature</p> <p><code>set_seed(seed: int) -&gt; None</code></p> <p>Parameters</p> <ul> <li>seed (<code>int</code>): The seed value used to initialize the random generators.</li> </ul> <p>Example</p> <pre><code># Set a specific seed to ensure reproducible results\nset_seed(42)\n</code></pre>"},{"location":"deep-promp/util/#plot_trajectories","title":"plot_trajectories()","text":"<p>The <code>plot_trajectories</code> function generates a plot for a list of labeled trajectories and saves it to a file. This function is designed to work with <code>SinusState</code> trajectories.</p> <p>Method Signature</p> <p><code>plot_trajectories(labeled_trajectories: List[Tuple[Trajectory[SinusState], str]], file_name: str, plot_title: str) -&gt; None</code></p> <p>Parameters</p> <ul> <li>labeled_trajectories (<code>List[Tuple[Trajectory, str]]</code>): A list of tuples, each containing a trajectory and its label.</li> <li>file_name (<code>str</code>): The name of the file where the plot will be saved.</li> <li>plot_title (<code>str</code>): Title of the plot.</li> </ul> <p>Example</p> <pre><code># Plot labeled trajectories and save the image\nplot_trajectories([(traj1, 'Trajectory 1'), (traj2, 'Trajectory 2')], 'trajectories_plot.png', 'Example Trajectories')\n</code></pre>"},{"location":"deep-promp/util/#generate_sin_trajectory","title":"generate_sin_trajectory()","text":"<p>The <code>generate_sin_trajectory</code> function creates a sinusoidal trajectory based on the specified amplitude, frequency, and number of points, returning it as a <code>Trajectory</code> object.</p> <p>Method Signature</p> <p><code>generate_sin_trajectory(amplitude: float | int, frequency: float | int, num_steps: int = 100) -&gt; Trajectory</code></p> <p>Parameters</p> <ul> <li>amplitude (<code>float</code> | <code>int</code>): The amplitude of the sinusoidal trajectory.</li> <li>frequency (<code>float</code> | <code>int</code>): The frequency of the sinusoidal trajectory.</li> <li>num_steps (<code>int</code>, optional): The number of points in the trajectory. Defaults to <code>100</code>.</li> </ul> <p>Returns</p> <ul> <li>Trajectory: A sinusoidal trajectory with <code>num_steps</code> points.</li> </ul> <p>Example</p> <pre><code># Generate a sinusoidal trajectory with amplitude 5 and frequency 1\ntrajectory = generate_sin_trajectory(amplitude=5, frequency=1)\n</code></pre>"},{"location":"deep-promp/util/#generate_sin_trajectory_set","title":"generate_sin_trajectory_set()","text":"<p>The <code>generate_sin_trajectory_set</code> function generates a set of sinusoidal trajectories with randomly selected amplitudes and frequencies from specified ranges.</p> <p>Method Signature</p> <p><code>generate_sin_trajectory_set(n: int, amplitude_range: Tuple[int, int] = (1, 10), frequency_range: Tuple[int, int] = (1, 1)) -&gt; List[Trajectory]</code></p> <p>Parameters</p> <ul> <li>n (<code>int</code>): The number of trajectories to generate.</li> <li>amplitude_range (<code>Tuple[int, int]</code>, optional): Range for amplitude values. Defaults to <code>(1, 10)</code>.</li> <li>frequency_range (<code>Tuple[int, int]</code>, optional): Range for frequency values. Defaults to <code>(1, 1)</code>.</li> </ul> <p>Returns</p> <ul> <li>List[Trajectory]: A list of generated sinusoidal trajectories.</li> </ul> <p>Example</p> <pre><code># Generate 5 sinusoidal trajectories with random amplitude and frequency within specified ranges\ntrajectories = generate_sin_trajectory_set(5, amplitude_range=(1, 5), frequency_range=(1, 2))\n</code></pre>"},{"location":"deep-promp/util/#generate_sin_trajectory_set_labeled","title":"generate_sin_trajectory_set_labeled()","text":"<p>The <code>generate_sin_trajectory_set_labeled</code> function generates a labeled set of sinusoidal trajectories with random amplitudes and frequencies within specified ranges.</p> <p>Method Signature</p> <p><code>generate_sin_trajectory_set_labeled(n: int, amplitude_range: Tuple[int, int] = (1, 10), frequency_range: Tuple[int, int] = (1, 1)) -&gt; List[dict]</code></p> <p>Parameters</p> <ul> <li>n (<code>int</code>): The number of trajectories to generate.</li> <li>amplitude_range (<code>Tuple[int, int]</code>, optional): Range for amplitude values. Defaults to <code>(1, 10)</code>.</li> <li>frequency_range (<code>Tuple[int, int]</code>, optional): Range for frequency values. Defaults to <code>(1, 1)</code>.</li> </ul> <p>Returns</p> <ul> <li>List[dict]: A list of dictionaries, each containing a trajectory, amplitude, and frequency. Example: <code>{'traj': traj, 'amplitude': 5, 'frequency': 1}</code>.</li> </ul> <p>Example</p> <pre><code># Generate 5 labeled sinusoidal trajectories with random amplitude and frequency within the specified ranges\nlabeled_trajectories = generate_sin_trajectory_set_labeled(5, amplitude_range=(1, 5), frequency_range=(1, 2))\n</code></pre>"},{"location":"interfaces/latent_decoder/","title":"LatentDecoder","text":"<p><code>moppy.interfaces.LatentDecoder</code></p> <p>The <code>LatentDecoder</code> class serves as an abstract base class for all latent decoder implementations. It defines the interface for decoding a latent variable into a normal distribution representing trajectory states.</p>"},{"location":"interfaces/latent_decoder/#decode_from_latent_variable","title":"decode_from_latent_variable()","text":"<p>Decodes a sampled latent variable into a normal distribution for each dimension of a <code>TrajectoryState</code>.</p> <p>Method Signature:</p> <p><code>decode_from_latent_variable(self, latent_variable, time)</code></p> <p>Parameters:</p> <ul> <li>latent_variable: A sampled latent variable \\( z \\) vector.</li> <li>time: The time step at which the decoding is performed.</li> </ul> <p>Returns:</p> <ul> <li>Normal Distribution: A normal distribution for each dimension of a <code>TrajectoryState</code>.</li> </ul> <p>Note: This method is abstract and must be implemented by subclasses.</p> <pre><code>@abstractmethod\ndef decode_from_latent_variable(self, latent_variable, time):\n    pass\n</code></pre>"},{"location":"interfaces/latent_encoder/","title":"LatentEncoder","text":"<p><code>moppy.interfaces.LatentEncoder</code></p> <p>The <code>LatentEncoder</code> class serves as an abstract base class for all latent encoder implementations. It defines the interface for encoding a trajectory into a latent variable and sampling from it.</p>"},{"location":"interfaces/latent_encoder/#functions","title":"Functions","text":""},{"location":"interfaces/latent_encoder/#encode_to_latent_variable","title":"encode_to_latent_variable()","text":"<p>Encodes a trajectory into a latent variable <code>z</code>, which is a vector of a fixed dimension.</p> <p>Method Signature:</p> <p><code>encode_to_latent_variable(self, trajectory: Trajectory)</code></p> <p>Parameters:</p> <ul> <li>trajectory (<code>Trajectory</code>): The trajectory to encode.</li> </ul> <p>Returns:</p> <ul> <li>Tuple: A tuple containing the sampled latent variable <code>z</code> represented as <code>mu</code> (mean) and <code>sigma</code> (standard deviation).</li> </ul> <p>Note: This method is abstract and must be implemented by subclasses.</p> <pre><code>@abstractmethod\ndef encode_to_latent_variable(self, trajectory: Trajectory):\n    pass\n</code></pre>"},{"location":"interfaces/latent_encoder/#sample_latent_variable","title":"sample_latent_variable()","text":"<p>Samples a latent variable <code>z</code> from a normal distribution specified by <code>mu</code> and <code>sigma</code>.</p> <p>Method Signature:</p> <p><code>sample_latent_variable(self, mu, sigma, percentage_of_standard_deviation=None)</code></p> <p>Parameters:</p> <ul> <li>mu: The mean of the normal distribution.</li> <li>sigma: The standard deviation of the normal distribution.</li> <li>percentage_of_standard_deviation (optional): The percentage of the standard deviation to sample from.</li> </ul> <p>Returns:</p> <ul> <li>Tensor: The sampled latent variable <code>z</code>.</li> </ul> <p>Note: This method is abstract and must be implemented by subclasses.</p> <pre><code>@abstractmethod\ndef sample_latent_variable(self, mu, sigma, percentage_of_standard_deviation=None):\n    pass\n</code></pre>"},{"location":"interfaces/movement_primitive/","title":"Movement Primitive","text":"<p><code>moppy.interfaces.movement_primitive.MovementPrimitive(self, name: str, encoder: LatentEncoder, decoder: LatentDecoder)</code></p> <p>The <code>MovementPrimitive</code> class serves as a base class for movement primitives, encapsulating the functionality of an encoder and decoder architecture. It provides a way to obtain a distribution of trajectory states based on a given context trajectory and a normalized time step.</p>"},{"location":"interfaces/movement_primitive/#parameters","title":"Parameters","text":"<ul> <li>name (<code>str</code>): The name of the movement primitive.</li> <li>encoder (<code>LatentEncoder</code>): The encoder used to encode the context trajectory into a latent variable.</li> <li>decoder (<code>LatentDecoder</code>): The decoder used to decode the latent variable back into trajectory states.</li> </ul>"},{"location":"interfaces/movement_primitive/#get_state_distribution_at","title":"get_state_distribution_at()","text":"<p>Obtains the state distribution for a given context trajectory at a specified normalized time step.</p> <p>Method Signature:</p> <p><code>get_state_distribution_at(self, context_trajectory: Trajectory, time: float)</code></p> <p>Parameters:</p> <ul> <li>context_trajectory (<code>Trajectory</code>): A context trajectory that serves as the input for encoding.</li> <li>time (<code>float</code>): A normalized time step between 0.0 and 1.0.</li> </ul> <p>Returns:</p> <ul> <li>Normal Distribution: A normal distribution for each dimension of a <code>TrajectoryState</code>.</li> </ul> <p>Raises:</p> <ul> <li>ValueError: If the context trajectory is <code>None</code>.</li> </ul>"},{"location":"interfaces/movement_primitive/#train","title":"train()","text":"<p>An abstract method intended for training the movement primitive with the given trajectories.</p> <p>Method Signature:</p> <p><code>train(self, trajectories: List[Trajectory])</code></p> <p>Parameters:</p> <ul> <li>trajectories (<code>List[Trajectory]</code>): A list of trajectories for training.</li> </ul> <p>Note: This method is abstract and should be implemented by subclasses.</p> <pre><code>@abstractmethod\ndef train(self, trajectories: List[Trajectory]):\n    pass\n</code></pre>"}]}